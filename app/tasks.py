# app/tasks.py
import time
import json
import os
import traceback
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from sqlalchemy import func, select, update
from app import scheduler, p115_client, APP_CONFIG, task_lock, http_client, app
from app.notifications import send_feishu_card_notification
from app.database import get_session, Shares

log = logging.getLogger(__name__)

STATUS_TEXT_MAP = {0: "å®¡æ ¸ä¸­", 1: "æ­£å¸¸", 2: "è¿è§„", 3: "å·²å¤±æ•ˆ", 4: "å·²å…¥åº“", 5: "å·²å®Œæˆ", 6: "æ¸…ç†å¤±è´¥"}

# --- ä»»åŠ¡è¾…åŠ©å‡½æ•° ---
async def _sync_to_cms(entry):
    cms_token = APP_CONFIG.get("cms_token")
    cms_domain = APP_CONFIG.get("cms_domain")
    if not (cms_token and cms_domain):
        return False, "æœªé…ç½®CMS"
    try:
        sync_url = f"{cms_domain.rstrip('/')}/api/sync/share115"
        headers = {'Authorization': f'Bearer {cms_token}', 'Content-Type': 'application/json'}
        path_hierarchy_json = entry.get("path_hierarchy", "[]")
        path_hierarchy = json.loads(path_hierarchy_json)
        folder_names = [item.get("name") for item in path_hierarchy if item.get("name")]
        base_path = APP_CONFIG.get('cms_sync_path', '/media/share/')
        p = Path(base_path)
        for name in folder_names:
            p = p / name
        local_path = p.as_posix()
        payload = {"share_code": entry.get("share_code"), "receive_code": entry.get("receive_code"), "cid": 0, "local_path": local_path}
        sync_response = await http_client.post(sync_url, json=payload, headers=headers)
        sync_response.raise_for_status()
        sync_data = sync_response.json()
        if sync_data.get("code") == 200:
            return True, "åŒæ­¥æˆåŠŸ"
        else:
            return False, sync_data.get("msg", "æœªçŸ¥CMSé”™è¯¯")
    except Exception as sync_e:
        return False, str(sync_e)

async def _trigger_cms_lift_sync():
    """æ ¹æ®é…ç½®è§¦å‘CMSçš„å¢é‡åŒæ­¥æ¥å£"""
    cms_domain = APP_CONFIG.get("cms_domain")
    cms_token = APP_CONFIG.get("cms_token")

    if not (cms_domain and cms_token):
        log.info("CMS åŸŸåæˆ– Token æœªé…ç½®ï¼Œè·³è¿‡æœ€ç»ˆçš„å¢é‡åŒæ­¥ã€‚")
        return

    # æ„é€ è¯·æ±‚ URL å’Œ Headers
    sync_url = f"{cms_domain.rstrip('/')}/api/sync/lift"
    headers = {"Authorization": f"Bearer {cms_token}"}
    
    log.info(f"æ­£åœ¨è§¦å‘ CMS å¢é‡åŒæ­¥æ¥å£: {sync_url}")
    try:
        # æ ¹æ®æ‚¨çš„curlå‘½ä»¤ï¼Œè¿™æ˜¯ä¸€ä¸ªGETè¯·æ±‚
        response = await http_client.get(sync_url, headers=headers, timeout=60) # å»¶é•¿è¶…æ—¶æ—¶é—´ä»¥å¤‡åŒæ­¥è€—æ—¶
        response.raise_for_status()
        log.info(f"âœ… CMS å¢é‡åŒæ­¥è¯·æ±‚å·²æˆåŠŸå‘é€, å“åº”: {response.text}")
    except Exception as e:
        log.error(f"âŒ å‘é€ CMS å¢é‡åŒæ­¥è¯·æ±‚å¤±è´¥: {e}", exc_info=True)

# --- æ ¸å¿ƒä»»åŠ¡å‡½æ•° ---
async def check_share_status_task():
    if not p115_client: return

    async with task_lock:
        log.info("ã€ä»»åŠ¡å¼€å§‹ã€‘åˆ†äº«çŠ¶æ€æ£€æŸ¥...")
        BATCH_SIZE = int(APP_CONFIG.get("task_batch_size", 5))
        API_SLEEP_INTERVAL = int(APP_CONFIG.get("task_api_sleep_seconds", 15))

        try:
            async with get_session() as session:
                query = select(Shares).filter(Shares.status.in_([0, 1])).limit(BATCH_SIZE)
                result = await session.execute(query)
                entries_to_process = [{c.name: getattr(row[0], c.name) for c in row[0].__table__.columns} for row in result.fetchall()]
        except Exception as e:
            log.error(f"è·å–å¾…å¤„ç†ä»»åŠ¡æ‰¹æ¬¡æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯", exc_info=True)
            return

        if not entries_to_process:
            log.info("æ²¡æœ‰éœ€è¦æ£€æŸ¥æˆ–åŒæ­¥çš„åˆ†äº«è®°å½•ã€‚")
            log.info("ã€ä»»åŠ¡ç»“æŸã€‘åˆ†äº«çŠ¶æ€æ£€æŸ¥å®Œæˆã€‚")
            return

        log.info(f"æœ¬æ¬¡éœ€å¤„ç† {len(entries_to_process)} ä¸ªåˆ†äº«ã€‚")

        for i, entry_data in enumerate(entries_to_process):
            try:
                current_status = entry_data['status']
                new_status = current_status

                if str(current_status) != '1':
                    log.debug(f"æ­£åœ¨æ£€æŸ¥åˆ†äº«: {entry_data.get('share_title', 'N/A')}")
                    info_data = await asyncio.to_thread(p115_client.share_info_app, {"share_code": entry_data["share_code"]})
                    if info_data.get("state"):
                        new_status = info_data.get("data", {}).get("share_state")
                    elif "å·²å–æ¶ˆ" in info_data.get('error', ''):
                        new_status = 3
                
                if str(new_status) == '1':
                    log.debug(f"æ­£åœ¨åŒæ­¥åˆ°CMS: {entry_data.get('share_title', 'N/A')}")
                    success, message = await _sync_to_cms(entry_data)
                    if success:
                        new_status = 4
                    else:
                        log.warning(f"åŒæ­¥å¤±è´¥: {message}")
                
                if new_status != current_status:
                    async with get_session() as session:
                        update_stmt = update(Shares).where(Shares.share_url == entry_data['share_url']).values(status=new_status)
                        await session.execute(update_stmt)
                    current_status_text = STATUS_TEXT_MAP.get(current_status, 'æœªçŸ¥')
                    new_status_text = STATUS_TEXT_MAP.get(new_status, 'æœªçŸ¥')
                    log.info(f"åˆ†äº«çŠ¶æ€å˜æ›´: '{entry_data.get('share_title')}' ä» [{current_status_text}] -> [{new_status_text}]")

                if i < len(entries_to_process) - 1:
                    log.debug(f"API è°ƒç”¨é—´éš”ä¼‘çœ  {API_SLEEP_INTERVAL} ç§’...")
                    await asyncio.sleep(API_SLEEP_INTERVAL)

            except Exception as e:
                log.error(f"å¤„ç†åˆ†äº« {entry_data.get('share_url')} æ—¶å¤±è´¥", exc_info=True)
                await send_feishu_card_notification(
                    "ã€âŒ å•é¡¹ä»»åŠ¡å¤±è´¥ã€‘115 æ™ºèƒ½ç®¡ç†å™¨å‘Šè­¦",
                    f"å¤„ç†åˆ†äº«`{entry_data.get('share_title')}`æ—¶å‘ç”Ÿé”™è¯¯:\n```{e}```",
                    "orange"
                )
                continue
        log.info("ã€ä»»åŠ¡ç»“æŸã€‘åˆ†äº«çŠ¶æ€æ£€æŸ¥å®Œæˆã€‚")

async def delete_synced_files_task_unit() -> bool:
    log.info("ğŸ§¹ å¼€å§‹æ‰§è¡Œä¸€ä¸ªæ¸…ç†æ‰¹æ¬¡...")
    BATCH_SIZE = int(APP_CONFIG.get("delete_task_batch_size", 10))
    # è·å–APIåˆ†å—å¤§å°
    CHUNK_SIZE = int(APP_CONFIG.get("delete_api_chunk_size", 230))

    async with get_session() as session:
        # 1. ä¸€æ¬¡æ€§è·å–æ‰¹æ¬¡å†…æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„ Shares å¯¹è±¡
        query = select(Shares).filter(Shares.status.in_([4, 6])).limit(BATCH_SIZE)
        result = await session.execute(query)
        entries_to_process = result.scalars().all()

        if not entries_to_process:
            log.info("âœ… æ²¡æœ‰å¾…æ¸…ç†è®°å½•ã€‚")
            return True

        log.info(f"æœ¬æ‰¹æ¬¡å¤„ç† {len(entries_to_process)} ä¸ªå¾…æ¸…ç†é¡¹ã€‚")

        # 2. èšåˆæ‰€æœ‰æ–‡ä»¶IDï¼Œå¹¶æŒ‰æƒ…å†µåˆ†ç±»
        all_file_ids_to_delete = []
        urls_with_files = []
        urls_without_files = []

        for entry in entries_to_process:
            try:
                file_ids = json.loads(entry.shared_cids) if entry.shared_cids else None
                if file_ids:
                    all_file_ids_to_delete.extend(file_ids)
                    urls_with_files.append(entry.share_url)
                else:
                    urls_without_files.append(entry.share_url)
            except (json.JSONDecodeError, TypeError):
                log.warning(f"è§£æåˆ†äº« '{entry.share_title}' çš„ cids å¤±è´¥ï¼Œè·³è¿‡æ­¤è®°å½•ã€‚")
                continue
        
        # 3. æ‰¹é‡æ›´æ–°æ²¡æœ‰æ–‡ä»¶IDçš„è®°å½•çŠ¶æ€
        if urls_without_files:
            update_stmt_no_files = update(Shares).where(
                Shares.share_url.in_(urls_without_files)
            ).values(status=5)
            await session.execute(update_stmt_no_files)
            log.info(f"{len(urls_without_files)} ä¸ªæ— æ–‡ä»¶IDçš„è®°å½•å·²ç›´æ¥æ ‡è®°ä¸ºå®Œæˆã€‚")

        # 4. å¦‚æœæœ‰æ–‡ä»¶éœ€è¦åˆ é™¤ï¼Œåˆ™åˆ†å—æ‰§è¡ŒAPIè°ƒç”¨
        if all_file_ids_to_delete:
            any_chunk_failed = False
            unique_file_ids = list(set(all_file_ids_to_delete)) # å»é‡ï¼Œé¿å…é‡å¤åˆ é™¤
            log.info(f"èšåˆåˆ° {len(unique_file_ids)} ä¸ªå”¯ä¸€æ–‡ä»¶IDéœ€è¦åˆ é™¤ã€‚")

            # å°†æ€»åˆ—è¡¨åˆ‡å‰²æˆå¤šä¸ªå¤§å°ä¸º CHUNK_SIZE çš„å­åˆ—è¡¨
            for i in range(0, len(unique_file_ids), CHUNK_SIZE):
                chunk = unique_file_ids[i:i + CHUNK_SIZE]
                log.debug(f"æ­£åœ¨å¤„ç†æ–‡ä»¶IDå— {i//CHUNK_SIZE + 1}ï¼ŒåŒ…å« {len(chunk)} ä¸ªæ–‡ä»¶ã€‚")
                
                delete_payload = {"file_ids": ",".join(map(str, chunk))}
                try:
                    delete_response = await asyncio.to_thread(p115_client.fs_delete_app, delete_payload)
                    if not delete_response.get("state"):
                        log.warning(f"â— å—åˆ é™¤å¤±è´¥: {delete_response.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        any_chunk_failed = True
                        break # ä¸€æ—¦æœ‰å—å¤±è´¥ï¼Œç«‹å³ä¸­æ–­åç»­åˆ é™¤æ“ä½œ
                except Exception as e:
                    log.error(f"â— å—åˆ é™¤æ—¶å‘ç”ŸAPIè¯·æ±‚å¼‚å¸¸", exc_info=True)
                    any_chunk_failed = True
                    break

            # 5. æ ¹æ®æ‰€æœ‰å—çš„æ‰§è¡Œç»“æœï¼Œç»Ÿä¸€æ›´æ–°æ•°æ®åº“çŠ¶æ€
            if any_chunk_failed:
                new_status = 6  # å¤±è´¥ -> æ¸…ç†å¤±è´¥
                log.warning(f"ç”±äºè‡³å°‘ä¸€ä¸ªAPIè¯·æ±‚å—å¤±è´¥ï¼Œæœ¬æ‰¹æ¬¡ {len(urls_with_files)} ä¸ªåˆ†äº«å‡æ ‡è®°ä¸ºæ¸…ç†å¤±è´¥ã€‚")
            else:
                new_status = 5  # æˆåŠŸ -> å·²å®Œæˆ
                log.info(f"âœ… æ‰€æœ‰æ–‡ä»¶å—å‡å·²æˆåŠŸåˆ é™¤ã€‚")
            
            update_stmt_with_files = update(Shares).where(
                Shares.share_url.in_(urls_with_files)
            ).values(status=new_status)
            await session.execute(update_stmt_with_files)

    # 6. è¿”å›æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å·²å¤„ç†å®Œæ¯•
    return len(entries_to_process) < BATCH_SIZE


async def master_cleanup_task():
    log.info(f"ã€ä»»åŠ¡å¼€å§‹ã€‘æ€»æ¸…ç†ä»»åŠ¡å¯åŠ¨...")
    if not p115_client: return
    
    async with task_lock:
        try:
            BATCH_INTERVAL_MINUTES = int(APP_CONFIG.get("delete_task_batch_interval_minutes", 5))
            
            log.info("æ‰§è¡Œé¦–ä¸ªæ¸…ç†æ‰¹æ¬¡...")
            all_done = await delete_synced_files_task_unit()

            while not all_done:
                log.debug(f"æ‰¹æ¬¡é—´ä¼‘çœ  {BATCH_INTERVAL_MINUTES} åˆ†é’Ÿ...")
                await asyncio.sleep(BATCH_INTERVAL_MINUTES * 60)
                
                log.info("æ‰§è¡Œä¸‹ä¸€ä¸ªæ¸…ç†æ‰¹æ¬¡...")
                all_done = await delete_synced_files_task_unit()
            
            log.info("æ‰€æœ‰æ¸…ç†æ‰¹æ¬¡å‡å·²å®Œæˆã€‚")
            log.info("å¼€å§‹æ‰§è¡Œæœ€ç»ˆçš„ Emby åˆ·æ–°ã€‚")
            await _trigger_cms_lift_sync()
            log.info("ã€ä»»åŠ¡ç»“æŸã€‘æ€»æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚")

        except Exception as e:
            log.error(f"æ€»æ¸…ç†ä»»åŠ¡æ‰§è¡Œå¤±è´¥", exc_info=True)
            error_msg = (
                f"**å¤±è´¥æ¨¡å—**: æ€»æ¸…ç†ä»»åŠ¡\n"
                f"**å¤±è´¥æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"**é”™è¯¯è¯¦æƒ…**: ```\n{traceback.format_exc()}\n```"
            )
            await send_feishu_card_notification("ã€âŒ ä»»åŠ¡å¤±è´¥ã€‘115 æ™ºèƒ½ç®¡ç†å™¨å‘Šè­¦", error_msg, "red")

async def send_daily_summary_task():
    log.info("ã€ä»»åŠ¡å¼€å§‹ã€‘ç”Ÿæˆå¹¶å‘é€æ¯æ—¥æ‘˜è¦...")
    status_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
    total_shares = 0
    try:
        async with get_session() as session:
            total_query = select(func.count()).select_from(Shares)
            total_result = await session.execute(total_query)
            total_shares = total_result.scalar_one()

            group_query = select(Shares.status, func.count(Shares.share_url)).group_by(Shares.status)
            result = await session.execute(group_query)
            for row in result.all():
                if row[0] in status_counts:
                    status_counts[row[0]] = row[1]
    except Exception as e:
        log.error(f"ç”Ÿæˆæ‘˜è¦æ—¶æ•°æ®åº“æŸ¥è¯¢å¤±è´¥", exc_info=True)
        return

    report_date = datetime.now().strftime('%Y-%m-%d')
    content = (
        f"**æŠ¥å‘Šæ—¥æœŸ**: {report_date}\n\n---\n\n"
        f"**åˆ†äº«æ€»è§ˆ**\n- ğŸ“ **æ€»è®¡**: {total_shares} æ¡\n\n"
        f"**çŠ¶æ€åˆ†å¸ƒ**\n"
        f"- â³ **å®¡æ ¸ä¸­**: {status_counts[0]} æ¡\n"
        f"- âœ… **æ­£å¸¸**: {status_counts[1]} æ¡\n"
        f"- ğŸ“¦ **å·²å…¥åº“**: {status_counts[4]} æ¡\n"
        f"- ğŸ‰ **å·²å®Œæˆ**: {status_counts[5]} æ¡\n"
        f"- ğŸš« **è¿è§„/å¤±æ•ˆ**: {status_counts[2] + status_counts[3]} æ¡\n"
        f"- âŒ **æ¸…ç†å¤±è´¥**: {status_counts.get(6, 0)} æ¡\n"
    )
    await send_feishu_card_notification(f"ã€ğŸ“Š æ¯æ—¥æŠ¥å‘Šã€‘115 æ™ºèƒ½ç®¡ç†å™¨", content, "blue")
    log.info("ã€ä»»åŠ¡ç»“æŸã€‘æ¯æ—¥æ‘˜è¦å·²å‘é€ã€‚")

def schedule_task():
    if scheduler.get_job('share_check_job'): scheduler.remove_job('share_check_job')
    if scheduler.get_job('delete_synced_job'): scheduler.remove_job('delete_synced_job')
    if scheduler.get_job('daily_summary_job'): scheduler.remove_job('daily_summary_job')

    if APP_CONFIG.get("task_enabled"):
        cron_str = APP_CONFIG.get('task_cron', '*/10 * * * *')
        try:
            cron_parts = cron_str.split()
            if len(cron_parts) != 5: raise ValueError("Cron è¡¨è¾¾å¼å¿…é¡»æœ‰5ä¸ªéƒ¨åˆ†")
            scheduler.add_job(check_share_status_task, 'cron', 
                              minute=cron_parts[0], hour=cron_parts[1], day=cron_parts[2], 
                              month=cron_parts[3], day_of_week=cron_parts[4], 
                              id='share_check_job')
            log.info(f"ğŸ•’ å®¡æ ¸ä»»åŠ¡å·²å¯ç”¨ï¼Œè®¡åˆ’: {cron_str}")
        except Exception as e:
            log.error(f"æ— æ³•è®¾ç½®å®¡æ ¸ä»»åŠ¡ï¼Œè¯·æ£€æŸ¥ Cron è¡¨è¾¾å¼ '{cron_str}': {e}")
    
    if APP_CONFIG.get("delete_task_enabled"):
        cron_str = APP_CONFIG.get('delete_task_cron', '0 4 * * *')
        try:
            cron_parts = cron_str.split()
            if len(cron_parts) != 5: raise ValueError("Cron è¡¨è¾¾å¼å¿…é¡»æœ‰5ä¸ªéƒ¨åˆ†")
            scheduler.add_job(master_cleanup_task, 'cron', 
                              minute=cron_parts[0], hour=cron_parts[1], day=cron_parts[2], 
                              month=cron_parts[3], day_of_week=cron_parts[4], 
                              id='delete_synced_job')
            log.info(f"ğŸ§¹ æ€»æ¸…ç†ä»»åŠ¡å·²å¯ç”¨ï¼Œè®¡åˆ’: {cron_str}")
        except Exception as e:
            log.error(f"æ— æ³•è®¾ç½®æ¸…ç†ä»»åŠ¡ï¼Œè¯·æ£€æŸ¥ Cron è¡¨è¾¾å¼ '{cron_str}': {e}")
    
    if APP_CONFIG.get("daily_summary_enabled"):
        cron_str = APP_CONFIG.get('summary_task_cron', '55 23 * * *')
        try:
            cron_parts = cron_str.split()
            if len(cron_parts) != 5: raise ValueError("Cron è¡¨è¾¾å¼å¿…é¡»æœ‰5ä¸ªéƒ¨åˆ†")
            scheduler.add_job(send_daily_summary_task, 'cron', 
                              minute=cron_parts[0], hour=cron_parts[1], day=cron_parts[2], 
                              month=cron_parts[3], day_of_week=cron_parts[4], 
                              id='daily_summary_job')
            log.info(f"ğŸ“Š æ¯æ—¥æ‘˜è¦ä»»åŠ¡å·²å¯ç”¨ï¼Œè®¡åˆ’: {cron_str}")
        except Exception as e:
            log.error(f"æ— æ³•è®¾ç½®æ¯æ—¥æ‘˜è¦ä»»åŠ¡ï¼Œè¯·æ£€æŸ¥ Cron è¡¨è¾¾å¼ '{cron_str}': {e}")
